
"""
Create a balanced ~N-qrel subset from the existing MS MARCO Doc v2 TREC-DL CSV subset generated by the other script.
Output: filtered CSVs in --out-dir with exactly --size qrels evenly split across all distinct relevance labels.
"""

import argparse
import os
import pandas as pd
import numpy as np


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in-dir", required=True)
    ap.add_argument("--out-dir", required=True)
    ap.add_argument("--size", type=int, default=1000)
    ap.add_argument("--seed", type=int, default=42)
    args = ap.parse_args()

    in_dir = args.in_dir
    out_dir = args.out_dir
    size = args.size
    seed = args.seed

    # read
    qrels = pd.read_csv(
        os.path.join(in_dir, "qrels.csv"),
        dtype={"query_id": str, "doc_id": str, "relevance": int, "iteration": str},
    )
    queries = pd.read_csv(
        os.path.join(in_dir, "queries.csv"),
        dtype={"query_id": str, "text": str, "dataset_id": str},
    )
    docs = pd.read_csv(
        os.path.join(in_dir, "docs.csv"),
        dtype={"doc_id": str, "url": str, "title": str, "body": str},
    )
    datasets = pd.read_csv(
        os.path.join(in_dir, "datasets.csv"),
        dtype={"dataset_id": str, "dataset_key": str},
    )

    # labels + even allocation (allow remainder)
    labels = sorted(qrels["relevance"].unique().tolist())
    k = len(labels)
    if k == 0:
        raise ValueError("No relevance labels found.")
    base = size // k
    rem = size % k
    targets = {lab: base for lab in labels}
    for lab in labels[:rem]:
        targets[lab] += 1

    # sample exactly targets per label (assume enough available)
    rng = np.random.default_rng(seed)
    parts = []
    for i, lab in enumerate(labels):
        df_lab = qrels[qrels["relevance"] == lab]
        take = targets[lab]
        # deterministic per label using derived seeds
        state = int(rng.integers(0, 2**32 - 1))
        parts.append(df_lab.sample(n=take, random_state=state))

    sampled_qrels = pd.concat(parts, axis=0, ignore_index=True)

    # filter related tables
    keep_qids = set(sampled_qrels["query_id"])
    keep_dids = set(sampled_qrels["doc_id"])
    queries_out = queries[queries["query_id"].isin(keep_qids)].copy()
    docs_out = docs[docs["doc_id"].isin(keep_dids)].copy()
    keep_dsids = set(queries_out["dataset_id"].unique())
    datasets_out = datasets[datasets["dataset_id"].isin(keep_dsids)].copy()

    # write
    os.makedirs(out_dir, exist_ok=True)
    sampled_qrels[["query_id", "doc_id", "relevance", "iteration"]].to_csv(
        os.path.join(out_dir, "qrels.csv"), index=False
    )
    queries_out[["query_id", "text", "dataset_id"]].to_csv(
        os.path.join(out_dir, "queries.csv"), index=False
    )
    docs_out[["doc_id", "url", "title", "body"]].to_csv(
        os.path.join(out_dir, "docs.csv"), index=False
    )
    datasets_out[["dataset_id", "dataset_key"]].to_csv(
        os.path.join(out_dir, "datasets.csv"), index=False
    )

    # quick summary
    dist = sampled_qrels["relevance"].value_counts().sort_index()
    print(f"Written to: {out_dir}")
    print(f"qrels: {len(sampled_qrels)} | labels: {labels}")
    print(dist.to_string())
    print(f"queries: {len(queries_out)} | docs: {len(docs_out)} | datasets: {len(datasets_out)}")


if __name__ == "__main__":
    main()
